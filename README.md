# 447

link for training data - https://huggingface.co/datasets/uonlp/CulturaX

models:
Multilingual m5 small/base:
https://github.com/google-research/multilingual-t5?tab=readme-ov-file#fine-tuning 

RWKV:
https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-1 

openELM:
https://huggingface.co/apple/OpenELM-450M 

Qwen 2.5B:
https://huggingface.co/Qwen/Qwen2.5-0.5B 
fine tune with unsloth: https://colab.research.google.com/drive/1Kose-ucXO1IBaZq5BvbwWieuubP7hxvQ?usp=sharing 

activate the venv with "source ./venv/bin/activate"
deactive the venv with deactivate




Potential models that we could use:
- https://huggingface.co/google-bert/bert-base-multilingual-cased
- https://huggingface.co/docs/transformers/en/model_doc/xlm-roberta
- https://huggingface.co/docs/transformers/en/model_doc/byt5

If we are using subtitles to train
- https://www.opensubtitles.org/en/search/subs
